{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a18a51e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-17T13:25:27.301943Z",
     "start_time": "2022-02-17T13:25:26.770238Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6e33af4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-17T13:25:28.476855Z",
     "start_time": "2022-02-17T13:25:28.467765Z"
    }
   },
   "outputs": [],
   "source": [
    "class CrossNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross Network\n",
    "    # layer_num是交叉网络的层数， input_dim表示输入的整体维度大小\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_num, input_dim):\n",
    "        super(CrossNetwork, self).__init__()\n",
    "        self.layer_num = layer_num\n",
    "        \n",
    "        # 定义网络层的参数\n",
    "        self.cross_weights = nn.ParameterList([\n",
    "            nn.Parameter(torch.rand(input_dim, 1))\n",
    "            for i in range(self.layer_num)\n",
    "        ])\n",
    "        self.cross_bias = nn.ParameterList([\n",
    "            nn.Parameter(torch.rand(input_dim, 1))\n",
    "            for i in range(self.layer_num)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x是(None, dim)的形状， 先扩展一个维度到(None, dim, 1)\n",
    "#         print(\"x.shape;{}\".format(x.shape))\n",
    "        x_0 = torch.unsqueeze(x, dim=2)\n",
    "\n",
    "        x = x_0.clone()\n",
    "        xT = x_0.clone().permute((0, 2, 1))     # （None, 1, dim) 将tensor的维度换位。\n",
    "\n",
    "        for i in range(self.layer_num):\n",
    "            x = torch.matmul(torch.bmm(x_0, xT), self.cross_weights[i]) + self.cross_bias[i] + x   # (None, dim, 1)\n",
    "            xT = x.clone().permute((0, 2, 1))   # (None, 1, dim)\n",
    "        \n",
    "        x = torch.squeeze(x)  # (None, dim)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf7d8cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-17T13:25:29.288693Z",
     "start_time": "2022-02-17T13:25:29.282820Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dnn(nn.Module):\n",
    "    \"\"\"\n",
    "    Dnn part\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_units, dropout=0.):\n",
    "        \"\"\"\n",
    "        hidden_units: 列表， 每个元素表示每一层的神经单元个数， 比如[256, 128, 64], 两层网络， 第一层神经单元128， 第二层64， 第一个维度是输入维度\n",
    "        dropout: 失活率\n",
    "        \"\"\"\n",
    "        super(Dnn, self).__init__()\n",
    "        \n",
    "        self.dnn_network = nn.ModuleList([nn.Linear(layer[0], layer[1]) for layer in list(zip(hidden_units[:-1], hidden_units[1:]))])\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        for linear in self.dnn_network:\n",
    "            x = linear(x)\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2fc199d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-17T13:25:30.144280Z",
     "start_time": "2022-02-17T13:25:30.133456Z"
    }
   },
   "outputs": [],
   "source": [
    "class DCN(nn.Module):\n",
    "    def __init__(self, feature_columns, hidden_units, layer_num, dnn_dropout=0.):\n",
    "        super(DCN, self).__init__()\n",
    "        self.dense_feature_cols, self.sparse_feature_cols = feature_columns\n",
    "        \n",
    "        # embedding \n",
    "        self.embed_layers = nn.ModuleDict({\n",
    "            'embed_' + str(i): nn.Embedding(num_embeddings=feat['feat_num'], embedding_dim=feat['embed_dim'])\n",
    "            for i, feat in enumerate(self.sparse_feature_cols)\n",
    "        })\n",
    "        \n",
    "        hidden_units.insert(0, len(self.dense_feature_cols) + len(self.sparse_feature_cols)*self.sparse_feature_cols[0]['embed_dim'])\n",
    "        self.dnn_network = Dnn(hidden_units)\n",
    "        self.cross_network = CrossNetwork(layer_num, hidden_units[0])         # layer_num是交叉网络的层数， hidden_units[0]表示输入的整体维度大小\n",
    "        self.final_linear = nn.Linear(hidden_units[-1]+hidden_units[0], 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        dense_input, sparse_inputs = x[:, :len(self.dense_feature_cols)], x[:, len(self.dense_feature_cols):]\n",
    "        sparse_inputs = sparse_inputs.long()\n",
    "        # 26*3*8\n",
    "        sparse_embeds = [self.embed_layers['embed_'+str(i)](sparse_inputs[:, i]) for i in range(sparse_inputs.shape[1])]\n",
    "\n",
    "        #3*208\n",
    "        sparse_embeds = torch.cat(sparse_embeds, axis=-1) \n",
    "    \n",
    "        x = torch.cat([sparse_embeds, dense_input], axis=-1)\n",
    "  \n",
    "        # cross Network\n",
    "        cross_out = self.cross_network(x)\n",
    "        print(\"cross_out.shapet:{}\".format(cross_out.shape))\n",
    "        # Deep Network\n",
    "        deep_out = self.dnn_network(x)\n",
    "        print(\"deep_out.shapet:{}\".format(deep_out.shape))\n",
    "        #  Concatenate\n",
    "        total_x = torch.cat([cross_out, deep_out], axis=-1)\n",
    "        \n",
    "        # out\n",
    "        outputs = F.sigmoid(self.final_linear(total_x))\n",
    "        \n",
    "        return outputs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732a7613",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
