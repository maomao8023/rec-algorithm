{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32e9d048",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-17T13:13:08.485553Z",
     "start_time": "2022-02-17T13:13:08.049965Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d5cefde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-17T13:13:10.509760Z",
     "start_time": "2022-02-17T13:13:10.502566Z"
    }
   },
   "outputs": [],
   "source": [
    "class FM(nn.Module):\n",
    "    \"\"\"FM part\"\"\"\n",
    "    def __init__(self, latent_dim, fea_num):\n",
    "        \"\"\"\n",
    "        latent_dim: 各个离散特征隐向量的维度 8\n",
    "        fea_num: 这个最后离散特征embedding之后的拼接和dense拼接的总特征个数 221\n",
    "        \"\"\"\n",
    "        super(FM, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        # 定义三个矩阵， 一个是全局偏置，一个是一阶权重矩阵， 一个是二阶交叉矩阵，注意这里的参数由于是可学习参数，需要用nn.Parameter进行定义\n",
    "        self.w0 = nn.Parameter(torch.zeros([1,]))\n",
    "        self.w1 = nn.Parameter(torch.rand([fea_num, 1]))\n",
    "        self.w2 = nn.Parameter(torch.rand([fea_num, latent_dim]))\n",
    "    \"\"\"inputs: 32*221\"\"\"\n",
    "    def forward(self, inputs):   \n",
    "        # 一阶交叉\n",
    "        first_order = self.w0 + torch.mm(inputs, self.w1)      # (samples_num, 1)\n",
    "        # 二阶交叉  这个用FM的最终化简公式\n",
    "        second_order = 1/2 * torch.sum(\n",
    "            torch.pow(torch.mm(inputs, self.w2), 2) - torch.mm(torch.pow(inputs,2), torch.pow(self.w2, 2)),\n",
    "            dim = 1,\n",
    "            keepdim = True\n",
    "        )         # (samples_num, 1)\n",
    "        \n",
    "        return first_order + second_order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e3a7468",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-17T13:13:20.424241Z",
     "start_time": "2022-02-17T13:13:20.418266Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dnn(nn.Module):\n",
    "    \"\"\"Dnn part\"\"\"\n",
    "    def __init__(self, hidden_units, dropout=0.):\n",
    "        \"\"\"\n",
    "        hidden_units: 列表， 每个元素表示每一层的神经单元个数， 比如[256, 128, 64], 两层网络， 第一层神经单元128， 第二层64， 第一个维度是输入维度\n",
    "        dropout = 0.\n",
    "        \"\"\"\n",
    "        super(Dnn, self).__init__()\n",
    "        \n",
    "        self.dnn_network = nn.ModuleList([nn.Linear(layer[0], layer[1]) for layer in list(zip(hidden_units[:-1], hidden_units[1:]))])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):  \n",
    "        for linear in self.dnn_network:\n",
    "            x = linear(x)\n",
    "            x = F.relu(x)    \n",
    "        x = self.dropout(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd2540a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-17T13:14:30.116703Z",
     "start_time": "2022-02-17T13:14:30.105285Z"
    }
   },
   "outputs": [],
   "source": [
    "class DeepFM(nn.Module):\n",
    "    def __init__(self, feature_columns, hidden_units, dnn_dropout=0.):\n",
    "        \"\"\"\n",
    "        DeepFM:\n",
    "        :param feature_columns: 特征信息， 这个传入的是fea_cols\n",
    "        :param hidden_units: 隐藏单元个数， 一个列表的形式， 列表的长度代表层数， 每个元素代表每一层神经元个数\n",
    "        \"\"\"\n",
    "        super(DeepFM, self).__init__()\n",
    "        self.dense_feature_cols, self.sparse_feature_cols = feature_columns\n",
    "        \n",
    "        # embedding\n",
    "        self.embed_layers = nn.ModuleDict({\n",
    "            'embed_' + str(i): nn.Embedding(num_embeddings=feat['feat_num'], embedding_dim=feat['embed_dim'])\n",
    "            for i, feat in enumerate(self.sparse_feature_cols)\n",
    "        })\n",
    "        \n",
    "        self.fea_num = len(self.dense_feature_cols) + len(self.sparse_feature_cols)*self.sparse_feature_cols[0]['embed_dim']\n",
    "        #print(\"fea_num:{}\".format(self.fea_num))\n",
    "        hidden_units.insert(0, self.fea_num)\n",
    "\n",
    "        self.fm = FM(self.sparse_feature_cols[0]['embed_dim'], self.fea_num)\n",
    "        self.dnn_network = Dnn(hidden_units, dnn_dropout)\n",
    "        self.nn_final_linear = nn.Linear(hidden_units[-1], 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        dense_inputs, sparse_inputs = x[:, :len(self.dense_feature_cols)], x[:, len(self.dense_feature_cols):]\n",
    "        sparse_inputs = sparse_inputs.long()       # 转成long类型才能作为nn.embedding的输入\n",
    "        sparse_embeds = [self.embed_layers['embed_'+str(i)](sparse_inputs[:, i]) for i in range(sparse_inputs.shape[1])]\n",
    "\n",
    "        sparse_embeds = torch.cat(sparse_embeds, dim=-1)\n",
    "      \n",
    "        # 把离散特征和连续特征进行拼接作为FM和DNN的输入\n",
    "        x = torch.cat([sparse_embeds, dense_inputs], dim=-1)\n",
    "\n",
    "        # Wide\n",
    "        wide_outputs = self.fm(x)\n",
    "\n",
    "        # deep\n",
    "        deep_outputs = self.nn_final_linear(self.dnn_network(x))\n",
    "        print(\"--------------------\")\n",
    "\n",
    "        # 模型的最后输出\n",
    "        outputs = F.sigmoid(torch.add(wide_outputs, deep_outputs))\n",
    "        print(\"outputs.shape:{}\".format((outputs.shape)))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d694d302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
